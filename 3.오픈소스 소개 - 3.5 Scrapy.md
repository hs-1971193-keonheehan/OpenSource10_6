### 3.오픈소스 소개 - 3.5 Scrapy

웹 사이트에서 필요한 데이터를 크롤링하고, 정보의 처리나 보완등의 애플리케이션에 사용할 수 있는 구조화된 데이터를 추출하는 오픈소스 프레임워크
https://github.com/scrapy/scrapy

- 가구조아 사용 목적 : 쇼핑사이트의 가구 데이터를 가져와 Elasticsearch에게 보낸다.
- 라이센스 : BSD-3-Clause 라이센스
- 프로그래밍 언어 : 파이썬
- 비동기 네트워킹 라이브러리 (Twisted)를 기반으로 작동한다.
 : 하나의 요청이 에러가 났을 때, 그 외의 로직들은 상관 없이 처리된다.-> 데이터 처리가 지연되지 않는다.

- 사용자가 원하는 방식대로 데이터가 가공되어 저장된다.
- xpath를 통해 복잡한 html도 파싱이 가능하다.
- 웹 스크래핑 기능과 웹 크롤링 기능으로 사용할 수 있다.
	1) 웹 스크래핑 : 특정 데이터 추출
	2) 웹 크롤링 : 모든 데이터 추출

- 구성 요소
	- 엔진 : 시스템의 모든 구성요소 간 데이터 흐름을 제어하고 특정 작업이 발생할 때 이벤트를 작동
	- 스케줄러 : 엔진에서 요청을 수신, 요청할 때 대기열에 넣는 역할
	- 다운로더 : 웹 페이지를 가져와서 엔진에 공급하는 역할
	- 스파이더 : 특정 웹 사이트에서 구조화된 데이터를 추출하는 방법을 포함해 스크랩하는 방법을 정의한 클래스
	- 아이템 파이프라인 : 스파이더에서 추출한 아이템을 처리하는 역할( 정리, 유효성 검사 등 )
	- 다운로더 미들웨어 : 엔진과 다운로더 사이에서 요청을 처리하는 특정 후크
	- 스파이더 미들웨어 : 엔진과 스파이더 사이에 있는 후크로 스파이더의 응답이나 요청을 처리

- 웹 크롤링 방법 
	1) 새로운 Scrapy 프로젝트를 만든다.
	2) 추출할 아이템을 정의한다.
	3) 스파이더를 작성한다.
	4) 아이템 파이프라인을 작성한다.

- 운영 매커니즘
![img](https://docs.scrapy.org/en/latest/_images/scrapy_architecture_02.png)

	- spider모듈로 웹 크롤링 요청을 받는다.
	- 데이터 요청 사항을 scheduler로 스케줄링하고 크롤링할 다음 요청을 요청한다.
	- scheduler가 engine에게 요청사항 반환한다.
	- engine이 downloader에게 요청사항을 전송한다.
	: Middleware를 통해 이동 (이때 Download하기 위한 데이터들의 여러 설정사항들이 적용된다.)
	- 인터넷으로부터 받아온 데이터들을 다시 Middleware를 통해 Engine에게 전송한다.
	- Engine이 응답 데이터를 Spider에게 보내고 이때 spidermiddleware를 통과한다.
	- Spider가 스크랩한 항목과 새로운 요청을 Engine에게 반환한다.
	- Engine이 Item pipeline으로 처리된 데이터를 보내고 다음 처리된 요청도 스케줄러로 보낸다. 그리고 다음 크롤링 요청을 받는다.
	( Item pipeline은 처리된 데이터를 Database나 Excel같은 데이터로 저장하기 위해 응답 데이터를 알맞게 가공하고 저장하게 된다.)
	- 스케줄러에 더 이상의 요청사항이 없을 때까지 프로세스가 반복하게 된다.
